{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Importaci√≥n de datos desde diferentes fuentes de datos\n",
    "\n",
    "#### Autores: Cristina G√≥mez Alonso, Carlos Tessier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 0. Importaci√≥n de las librer√≠a Pandas y os de Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`pandas` es una librer√≠a de Python especializada en el **an√°lisis y manipulaci√≥n de datos**, que proporciona estructuras como **Series** (unidimensional) y **DataFrame** (tablas tipo Excel) para trabajar de forma r√°pida y eficiente con datos estructurados. üöÄ\n",
    "\n",
    "\n",
    "La librer√≠a `os` de Python permite interactuar con el sistema operativo para manejar rutas, archivos, directorios y variables de entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Consulta de datasets precargados\n",
    "Existen datasets  dentro de la librer√≠a scikit-learn que podemos cargar de forma simple para hacer pruebas\n",
    "\n",
    "El **dataset Iris** que viene en **scikit-learn** es uno de los conjuntos de datos m√°s cl√°sicos y utilizados en **aprendizaje autom√°tico** y **estad√≠stica**.\n",
    "\n",
    "Sirve principalmente como **ejemplo educativo** y de **prueba de algoritmos** porque es peque√±o, f√°cil de entender y ya viene preprocesado.\n",
    "\n",
    "### Caracter√≠sticas principales:\n",
    "\n",
    "* Contiene **150 muestras** de flores de iris.\n",
    "* Cada muestra tiene **4 caracter√≠sticas num√©ricas**:\n",
    "\n",
    "  1. Largo del s√©palo (sepal length)\n",
    "  2. Ancho del s√©palo (sepal width)\n",
    "  3. Largo del p√©talo (petal length)\n",
    "  4. Ancho del p√©talo (petal width)\n",
    "* Las muestras pertenecen a **3 clases** (50 ejemplos de cada una):\n",
    "\n",
    "  * *Iris setosa*\n",
    "  * *Iris versicolor*\n",
    "  * *Iris virginica*\n",
    "\n",
    "### ¬øPara qu√© se usa?\n",
    "\n",
    "* **Clasificaci√≥n**: entrenar modelos para predecir la especie de una flor seg√∫n sus medidas.\n",
    "* **Visualizaci√≥n**: como solo tiene 4 variables, se presta muy bien a gr√°ficos 2D y 3D.\n",
    "* **Pr√°ctica con algoritmos**: regresi√≥n log√≠stica, k-NN, SVM, √°rboles de decisi√≥n, redes neuronales, etc.\n",
    "* **Evaluaci√≥n de t√©cnicas de preprocesamiento**: normalizaci√≥n, reducci√≥n de dimensionalidad (PCA), validaci√≥n cruzada, etc.\n",
    "\n",
    "En pocas palabras: el dataset Iris es un **\"hola mundo\" del machine learning** üèµÔ∏è.\n",
    "\n",
    "¬øQuieres que te muestre un ejemplo pr√°ctico en Python con `scikit-learn`, cargando el dataset y entrenando un modelo simple?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# 1. Cargar el dataset Iris\n",
    "iris_data = load_iris()\n",
    "iris_data.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Un **DataFrame** es una **tabla de datos en Python** (de la librer√≠a **pandas**) que se parece a una hoja de Excel:\n",
    "\n",
    "* Tiene **filas** (observaciones, registros).\n",
    "* Tiene **columnas** (variables, caracter√≠sticas).\n",
    "* Cada columna puede tener un tipo de dato diferente (n√∫meros, texto, fechas, etc.).\n",
    "\n",
    "üëâ Es la estructura m√°s usada en an√°lisis de datos porque permite **organizar, filtrar, calcular y visualizar datos** de forma muy sencilla.\n",
    "\n",
    "Vamos a convertir el dataset Iris en un DataFrame de pandas, a√±adiendo tambi√©n la columna con el nombre de la especie en texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Crear un DataFrame con las caracter√≠sticas (columnas: s√©palo y p√©talo)\n",
    "iris = pd.DataFrame(iris_data.data,columns=iris_data.feature_names)\n",
    "\n",
    "# Agregar una nueva columna 'species' con el nombre de la especie\n",
    "iris['species'] = pd.Series(iris_data.target_names[iris_data.target])\n",
    "\n",
    "# Mostrar las primeras 5 filas de la tabla para ver c√≥mo queda\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a dibujar un **gr√°fico de dispersi√≥n** (scatter plot) usando el **DataFrame `iris`** que creamos antes.\n",
    "\n",
    "* Este gr√°fico es muy √∫til porque el **largo y ancho del p√©talo** son las variables que mejor diferencian a las especies.\n",
    "* Ver√°s que **Iris setosa** queda separada claramente de las otras dos especies en esta proyecci√≥n.\n",
    "* En cambio, **versicolor** y **virginica** se solapan un poco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Creamos un grafico de dispersi√≥n (scatter plot) usando el DataFrame iris creado antes\n",
    "iris.plot.scatter(x='petal length (cm)',\n",
    "                  y='petal width (cm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"background-color:green;\">\n",
    "\n",
    "## *Ejercicio 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"background-color:green;\">\n",
    "\n",
    "Consulta la librer√≠a wine. ¬øDe qu√© trata? ¬øQu√© columnas tiene? Muestra un gr√°fico de puntos de dos features relevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To-do\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Lectura de datos desde un fichero CSV\n",
    "\n",
    "\n",
    "`pd.read_csv(...)`: Es una funci√≥n de **pandas** que lee un archivo en formato **CSV** (valores separados por comas).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Librer√≠a para trabajar con DataFrames (tablas de datos)\n",
    "\n",
    "ruta_csv =  \"data/CSV_EX_1.csv\"\n",
    "\n",
    "# Leemos el archivo CSV y lo cargamos en un DataFrame de pandas\n",
    "df1 = pd.read_csv(ruta_csv)\n",
    "\n",
    "# Mostramos el DataFrame para ver el contenido\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV con las cabeceras desconocidas (missing headers)\n",
    "\n",
    "\n",
    "Cuando lees un archivo **CSV sin cabeceras** (es decir, no tiene fila de t√≠tulos de columnas), `pandas` por defecto **interpretar√° la primera fila como si fueran nombres de columnas**, lo que puede ser un problema porque en realidad son datos.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ruta_csv2 = \"data/CSV_EX_2.csv\"\n",
    "\n",
    "df2 = pd.read_csv(ruta_csv2)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando lees un archivo **CSV sin cabeceras** (es decir, no tiene fila de t√≠tulos de columnas), `pandas` por defecto **interpretar√° la primera fila como si fueran nombres de columnas**, lo que puede ser un problema porque en realidad son datos.\n",
    "\n",
    "\n",
    "### üîπ Soluci√≥n: usar `header=None`\n",
    "\n",
    "Si no tienes cabeceras, debes indicarlo expl√≠citamente:\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(\"datos.csv\", header=None)\n",
    "print(df)\n",
    "```\n",
    "\n",
    "* Se crean nombres de columna autom√°ticos: `0, 1, 2, ...`\n",
    "* Ahora los datos est√°n intactos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_csv2 = \"data/CSV_EX_2.csv\"\n",
    "\n",
    "df2 = pd.read_csv(ruta_csv2, header=None)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A√±adir nombres personalizados\n",
    "\n",
    "Puedes poner tus propios nombres de columnas con `names`:\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(\"datos.csv\", header=None, names=[\"A\", \"B\", \"C\"])\n",
    "print(df)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_csv2 =  \"data/CSV_EX_2.csv\"\n",
    "\n",
    "df2 = pd.read_csv(ruta_csv2, header=None, names=['Bedroom','Sq.ft','Locality','Price($)'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  CSV con separadores (delimiters) que no son comas\n",
    "\n",
    "Por defecto, **`pandas.read_csv()` usa la coma (`,`) como separador** ‚úÖ\n",
    "\n",
    "Es decir, si no le pasas el argumento `sep`, asume que el archivo es un **CSV est√°ndar** (Comma-Separated Values).\n",
    "\n",
    "\n",
    "üîπ Si el archivo en realidad usa otro separador (como `;`, `\\t`, `|`), pero no lo indicas, el DataFrame probablemente te salga con **una sola columna** (toda la fila le√≠da como texto).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_csv3 = \"data/CSV_EX_3.csv\"\n",
    "\n",
    "df3 = pd.read_csv(ruta_csv3)\n",
    "df3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_csv3 =  \"data/CSV_EX_3.csv\"\n",
    "\n",
    "# Indicamos expl√≠citamente que el separador (delimiter) es el punto y coma (;)\n",
    "df3 = pd.read_csv(ruta_csv3,sep=';')\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sustituci√≥n de cabeceras\n",
    "\n",
    "Si ya tiene una cabecera y llamamos a names, en vez de sustituir la cabecera original tendr√°s la fila de texto duplicada en los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_csv1 =  \"data/CSV_EX_1.csv\"\n",
    "\n",
    "df4 = pd.read_csv(ruta_csv1,names=['A','B','C','D'])\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_csv1 =  \"data/CSV_EX_1.csv\"\n",
    "\n",
    " # Leemos usando la primera fila como cabecera\n",
    "df4 = pd.read_csv(ruta_csv1,header=0,names=['A','B','C','D']) \n",
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sustracci√≥n de las filas iniciales (Skip initial rows)\n",
    "\n",
    "A veces los archivos CSV tienen filas de informaci√≥n al inicio que no pertenecen a la tabla (ejemplo: \"Filetype: CSV\" o \"Info about some houses\").\n",
    "\n",
    "Por defecto, pandas.read_csv() intenta leer desde la primera fila, por eso esas l√≠neas aparecen en el DataFrame como basura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_csv =  \"data/CSV_EX_skiprows.csv\"\n",
    "\n",
    "\n",
    "df5 = pd.read_csv(ruta_csv)\n",
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### üîπ Soluci√≥n: usar `skiprows`\n",
    "\n",
    "Con el argumento `skiprows` puedes indicarle a pandas **cu√°ntas filas debe saltar al inicio**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_csv =  \"data/CSV_EX_skiprows.csv\"\n",
    "\n",
    "df5 = pd.read_csv(ruta_csv,skiprows=2)\n",
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sustracci√≥n de las √∫ltimas filas (Skip footers)\n",
    "\n",
    "Algunos archivos pueden tener tanto ‚Äúruido‚Äù tanto al inicio como al final, como en este ejemplo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_csv = \"data/CSV_EX_skipfooter.csv\"\n",
    "\n",
    "df6 = pd.read_csv(ruta_csv)\n",
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adem√°s de `skiprows` (para saltar filas al inicio), en `pandas.read_csv()` tambi√©n puedes usar **`skipfooter`** para eliminar filas al final del archivo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_csv = \"data/CSV_EX_skipfooter.csv\"\n",
    "\n",
    "df6 = pd.read_csv(ruta_csv,skiprows=2,skipfooter=1,engine='python')\n",
    "df6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de las primeras *n* filas\n",
    "\n",
    "Cuando trabajas con **CSV muy grandes**, a veces no quieres cargarlos completos en memoria (pueden pesar gigas).\n",
    "Para eso, `pandas.read_csv()` tiene el par√°metro **`nrows`**, que permite leer solo las primeras *n* filas.\n",
    "\n",
    "---\n",
    "\n",
    "Esto es especialmente √∫til para:\n",
    "\n",
    "* **Inspeccionar la estructura** del archivo antes de procesarlo entero.\n",
    "* **Prototipar an√°lisis r√°pidos** con datasets muy grandes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_csv =  \"data/CSV_EX_1.csv\"\n",
    "\n",
    "\n",
    "df7 = pd.read_csv(ruta_csv,nrows=2)\n",
    "df7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sustracci√≥n de l√≠neas en blanco con skip_blank_lines\n",
    "\n",
    "En `pandas.read_csv()` existe el par√°metro **`skip_blank_lines`**, que sirve  para **ignorar l√≠neas en blanco** dentro de un CSV.\n",
    "\n",
    "Por defecto est√° en `True`, es decir, pandas **ya elimina autom√°ticamente las filas vac√≠as**.\n",
    "Si lo pones en `False`, las l√≠neas en blanco se conservar√°n y se mostrar√°n como filas llenas de `NaN`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_csv =  \"data/CSV_EX_blankline.csv\"\n",
    "\n",
    "\n",
    "df9 = pd.read_csv(ruta_csv)\n",
    "df9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_csv =  \"data/CSV_EX_blankline.csv\"\n",
    "\n",
    "df9 = pd.read_csv(ruta_csv,skip_blank_lines=False)\n",
    "df9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de un CSV desde un fichero comprimido \n",
    "\n",
    "\n",
    "Una de las ventajas de **pandas** es que puede leer archivos CSV **directamente desde ficheros comprimidos** como `.zip`, `.gz`, `.bz2` o `.xz` sin que tengas que descomprimirlos manualmente.\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(\"datos.zip\", compression=\"zip\")\n",
    "```\n",
    "\n",
    "Pandas detecta el tipo de compresi√≥n autom√°ticamente en muchos casos, as√≠ que en muchos entornos **no necesitas poner `compression`**:\n",
    "\n",
    "Si el `.zip` contiene **m√°s de un archivo**, debes usar `zipfile` o `pyzipper` para listar y seleccionar el que quieres leer:\n",
    "\n",
    "\n",
    "```python\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"datos.zip\", \"r\") as z:\n",
    "    print(z.namelist())  # Lista de archivos dentro del ZIP\n",
    "    with z.open(\"CSV_EX_1.csv\") as f:   # abrimos  el archivo\n",
    "        df = pd.read_csv(f)\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_csv = \"data/CSV_EX_1.zip\"\n",
    "\n",
    "df10 = pd.read_csv(ruta_csv)\n",
    "df10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div style=\"background-color:green;\">\n",
    "\n",
    "## *Ejercicio 2*\n",
    "\n",
    "Consulta la web oficial de [Datos abertos de la junta de Castilla y Le√≥n](https://analisis.datosabiertos.jcyl.es/explore/?sort=modified) e intenta cargar un dataset de tu inter√©s en formato CSV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To-do\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Lectura de datos desde un fichero Excel (uso de sheet_name)\n",
    "\n",
    "Cuando trabajamos con **ficheros Excel**, no siempre nos encontramos con un √∫nico conjunto de datos. Muchas veces un archivo contiene **varias hojas** (por ejemplo, ‚ÄúEnero‚Äù, ‚ÄúFebrero‚Äù, ‚ÄúMarzo‚Äù) y queremos analizarlas todas en Python.\n",
    "\n",
    "La librer√≠a **pandas** nos permite hacerlo f√°cilmente con la funci√≥n `pd.read_excel()`.\n",
    "\n",
    "---\n",
    "\n",
    "## Lectura de una hoja concreta\n",
    "\n",
    "Lo m√°s habitual es leer una sola hoja especificando el nombre o √≠ndice:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar la hoja llamada \"Ventas\"\n",
    "df = pd.read_excel(\"datos.xlsx\", sheet_name=\"Ventas\")\n",
    "\n",
    "# O la primera hoja (√≠ndice 0)\n",
    "df = pd.read_excel(\"datos.xlsx\", sheet_name=0)\n",
    "```\n",
    "\n",
    "Esto devuelve un √∫nico **DataFrame**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install openpyxl\n",
    "\n",
    "ruta_csv = \"data/Housing_data.xlsx\"\n",
    "\n",
    "\n",
    "df11_1 = pd.read_excel(ruta_csv,sheet_name='Data_Tab_1')\n",
    "df11_2 = pd.read_excel(ruta_csv,sheet_name='Data_Tab_2')\n",
    "df11_3 = pd.read_excel(ruta_csv,sheet_name='Data_Tab_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Filas y columnas de la hoja\n",
    "df11_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df11_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df11_3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Lectura de todas las hojas con `sheet_name=None`\n",
    "\n",
    "Si queremos **leer todas las hojas de un Excel de una sola vez**, basta con pasar `sheet_name=None`:\n",
    "\n",
    "```python\n",
    "all_sheets = pd.read_excel(\"datos.xlsx\", sheet_name=None)\n",
    "```\n",
    "\n",
    "En este caso, lo que se devuelve es un **diccionario ordenado** (`OrderedDict`) en el que:\n",
    "\n",
    "* Cada **clave** es el nombre de la hoja del Excel.\n",
    "* Cada **valor** es un **DataFrame** con los datos de esa hoja.\n",
    "\n",
    "---\n",
    "\n",
    "## Ejemplo pr√°ctico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ruta_csv = \"data/Housing_data.xlsx\"\n",
    "\n",
    "dict_df = pd.read_excel(ruta_csv,sheet_name=None)\n",
    "\n",
    "# Ver los nombres de las hojas\n",
    "print(dict_df.keys())\n",
    "\n",
    "# Acceder al DataFrame de \"Febrero\"\n",
    "Data_Tab_2 = dict_df[\"Data_Tab_2\"]\n",
    "\n",
    "# Ver las primeras filas\n",
    "print(Data_Tab_2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Iterar sobre todas las hojas\n",
    "\n",
    "Podemos recorrer el diccionario y trabajar con cada hoja:\n",
    "\n",
    "\n",
    "## Conclusi√≥n\n",
    "\n",
    "El par√°metro `sheet_name=None` es muy √∫til cuando necesitamos **cargar varias hojas de Excel de una sola vez**. Nos evita tener que llamar a `pd.read_excel()` repetidamente y nos da un acceso m√°s organizado a los datos, ya que podemos trabajar con cada hoja como si fuera un **DataFrame** dentro de un diccionario.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for nombre, df in dict_df.items():\n",
    "    print(f\"Hoja: {nombre}, Tama√±o: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div style=\"background-color:green;\">\n",
    "\n",
    "## *Ejercicio 3*\n",
    "\n",
    "Consulta el mismo dataset de [Datos abertos de la junta de Castilla y Le√≥n](https://analisis.datosabiertos.jcyl.es/explore/?sort=modified), pero ahora c√°rgalo desde un fichero Excel. Si no buscar otro que si tenga opci√≥n de cargarlo en excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. Lectura de datos en un fichero delimitado TXT\n",
    "\n",
    "Los ficheros de texto **delimitados** (por tabulador, espacio, `|`, etc.) se leen en **pandas** igual que un CSV, solo que cambiando el par√°metro `sep` en `pd.read_csv()`.\n",
    "\n",
    "---\n",
    "\n",
    "Ejemplo con tabulador (`\\t`)\n",
    "\n",
    "Si tu archivo TXT usa **tabulaciones** para separar columnas:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"incendios.txt\", sep=\"\\t\")\n",
    "\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Ejemplo con barra vertical (`|`)\n",
    "\n",
    "Si los datos est√°n separados por `|`:\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(\"incendios.txt\", sep=\"|\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    " Ejemplo con espacio(s)\n",
    "\n",
    "Si los campos est√°n separados por **uno o m√°s espacios**:\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(\"incendios.txt\", delim_whitespace=True)\n",
    "```\n",
    "\n",
    "(`delim_whitespace=True` detecta autom√°ticamente bloques de espacios como delimitador).\n",
    "\n",
    "---\n",
    "\n",
    "## pandas.read_table()\n",
    "\n",
    "`pandas.read_table()` es b√°sicamente un **atajo** de `pd.read_csv()` donde el separador por defecto es el **tabulador** (`\\t`).\n",
    "Esto es muy √∫til cuando trabajamos con ficheros `.txt` delimitados por tabulaciones.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Si el separador es otro (ej. `|` o `;`)\n",
    "\n",
    "Tambi√©n puedes usar `read_table` especificando `sep`:\n",
    "\n",
    "```python\n",
    "# Con barra vertical\n",
    "df = pd.read_table(\"incendios.txt\", sep=\"|\")\n",
    "\n",
    "# Con punto y coma\n",
    "df = pd.read_table(\"incendios.txt\", sep=\";\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Con nombres de columnas personalizados\n",
    "\n",
    "```python\n",
    "df = pd.read_table(\"incendios.txt\", sep=\"\\t\", header=None,\n",
    "                   names=[\"Fecha\", \"Provincia\", \"Hectareas\"])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ruta_txt =  \"data/Table_EX_1.txt\"\n",
    "\n",
    "\n",
    "df13 = pd.read_table(ruta_txt)\n",
    "df13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ruta_txt =  \"data/Table_EX_1.txt\"\n",
    "\n",
    "\n",
    "df13 = pd.read_table(ruta_txt,sep=',')\n",
    "df13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ruta_txt = \"data/Table_tab_separated.txt\"\n",
    "\n",
    "df13 = pd.read_table(ruta_txt,)\n",
    "df13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"background-color:green;\">\n",
    "\n",
    "## *Ejercicio 4*\n",
    "Consulta ahora la web oficial del INE (Instituto Nacional de Estad√≠stica) y descarga y carga un fichero plano TXT con los datos sobre la [poblaci√≥n por provincias](https://www.ine.es/dynt3/inebase/es/index.htm?padre=517&capsel=522)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lectura de tablas HTML desde una URL y ajustes para encontrar la tabla deseada\n",
    "\n",
    "Con `pandas` puedes leer tablas HTML directamente desde una URL con `pd.read_html`. En p√°ginas como la de Wikipedia suele haber **varias tablas**, as√≠ que conviene usar algunos ‚Äúajustes‚Äù para quedarnos con la **tabla de medallas** concreta.\n",
    "\n",
    "## Opci√≥n r√°pida (inspeccionar y elegir)\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/2024_Summer_Olympics_medal_table\"\n",
    "\n",
    "# Lee TODAS las tablas que pandas encuentre\n",
    "tablas = pd.read_html(url)\n",
    "\n",
    "# Echa un vistazo a cu√°ntas hay y a sus cabeceras\n",
    "print(len(tablas))\n",
    "for i, t in enumerate(tablas):\n",
    "    print(i, list(t.columns)[:6])\n",
    "```\n",
    "\n",
    "Con esto ves el √≠ndice de cada tabla y sus columnas para elegir la que te interesa:\n",
    "\n",
    "```python\n",
    "df = tablas[√çNDICE_QUE_CORRESPONDA]\n",
    "```\n",
    "\n",
    "## Opci√≥n filtrando por el contenido de columnas (recomendada)\n",
    "\n",
    "Wikipedia suele usar columnas tipo **Gold, Silver, Bronze, Total**. Podemos pedir solo tablas que contengan esos nombres:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/2024_Summer_Olympics_medal_table\"\n",
    "\n",
    "# match acepta regex; buscamos una tabla con estas columnas t√≠picas\n",
    "dfs = pd.read_html(url, match=re.compile(r\"Gold|Silver|Bronze|Total\"))\n",
    "\n",
    "# Suele devolver 1 tabla (la principal). Si hay m√°s, nos quedamos con la que tenga todas las columnas\n",
    "def es_tabla_medallas(x):\n",
    "    cols = set([str(c).lower() for c in x.columns])\n",
    "    return {\"gold\",\"silver\",\"bronze\"}.issubset(cols) or {\"gold\",\"silver\",\"bronze\",\"total\"}.issubset(cols)\n",
    "\n",
    "df = next((x for x in dfs if es_tabla_medallas(x)), dfs[0])\n",
    "\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "## Opci√≥n por atributos HTML (cuando conoces la clase)\n",
    "\n",
    "Muchas tablas de Wikipedia tienen clase `wikitable sortable`. Puedes usarla:\n",
    "\n",
    "```python\n",
    "dfs = pd.read_html(url, attrs={\"class\": \"wikitable\"})\n",
    "# ahora filtra por columnas como arriba:\n",
    "df = next((x for x in dfs if es_tabla_medallas(x)), dfs[0])\n",
    "```\n",
    "\n",
    "## Limpieza t√≠pica\n",
    "\n",
    "Las tablas de Wikipedia traen notas al pie, filas de totales, s√≠mbolos, etc. Aqu√≠ un bloque para dejar la tabla lista para an√°lisis:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Renombrar columnas t√≠picas a un formato homog√©neo\n",
    "ren = {c:str(c).strip().lower() for c in df.columns}\n",
    "df.columns = [ren[c] for c in df.columns]\n",
    "\n",
    "# Algunas versiones usan 'noc' o 'nation'\n",
    "col_pais = \"noc\" if \"noc\" in df.columns else (\"nation\" if \"nation\" in df.columns else None)\n",
    "if col_pais is None:\n",
    "    # en ocasiones hay multi-√≠ndice, intenta aplanarlo:\n",
    "    df.columns = [c[-1].lower() if isinstance(c, tuple) else str(c).lower() for c in df.columns]\n",
    "    col_pais = \"noc\" if \"noc\" in df.columns else (\"nation\" if \"nation\" in df.columns else df.columns[1])\n",
    "\n",
    "# Quitar filas que no son pa√≠ses (totales, etc.)\n",
    "filt_no_pais = df[col_pais].astype(str).str.contains(\"total\", case=False) | df[col_pais].isna()\n",
    "df = df[~filt_no_pais].copy()\n",
    "\n",
    "# Columnas num√©ricas que nos interesan\n",
    "num_cols = [c for c in [\"gold\",\"silver\",\"bronze\",\"total\"] if c in df.columns]\n",
    "\n",
    "# Eliminar notas/super√≠ndices tipo ‚Äú20[a]‚Äù y convertir a n√∫mero\n",
    "for c in num_cols:\n",
    "    df[c] = (\n",
    "        df[c]\n",
    "        .astype(str)\n",
    "        .str.replace(r\"[^\\d\\.\\-]\", \"\", regex=True)   # quita caracteres no num√©ricos\n",
    "        .replace({\"\": np.nan})\n",
    "        .astype(float)\n",
    "    )\n",
    "\n",
    "# A veces hay columna 'rank' con notas; la limpiamos si existe\n",
    "if \"rank\" in df.columns:\n",
    "    df[\"rank\"] = (\n",
    "        df[\"rank\"].astype(str)\n",
    "        .str.replace(r\"[^\\d]\", \"\", regex=True)\n",
    "        .replace({\"\": np.nan})\n",
    "        .astype(float)\n",
    "    )\n",
    "\n",
    "# Resultado limpio\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "## Ejemplos de an√°lisis en 1‚Äì2 l√≠neas (sin matplotlib expl√≠cito)\n",
    "\n",
    "Top 10 por oros:\n",
    "\n",
    "```python\n",
    "(df.sort_values(\"gold\", ascending=False)\n",
    "   .head(10)[[col_pais, \"gold\"]]\n",
    "   .plot(kind=\"bar\", x=col_pais, y=\"gold\", title=\"Top 10 por oros\"))\n",
    "```\n",
    "\n",
    "Top 10 por total de medallas:\n",
    "\n",
    "```python\n",
    "(df.sort_values(\"total\", ascending=False)\n",
    "   .head(10)[[col_pais, \"total\"]]\n",
    "   .plot(kind=\"barh\", x=col_pais, y=\"total\", title=\"Top 10 por total de medallas\"))\n",
    "```\n",
    "\n",
    "Distribuci√≥n de medallas (tarta) para un pa√≠s concreto:\n",
    "\n",
    "```python\n",
    "pais = \"Spain\"  # cambia por el pa√≠s que te interese\n",
    "fila = df[df[col_pais].str.contains(pais, case=False, na=False)][num_cols].sum()\n",
    "fila.plot(kind=\"pie\", autopct=\"%1.1f%%\", title=f\"Distribuci√≥n de medallas - {pais}\")\n",
    "```\n",
    "\n",
    "## Guardar a CSV\n",
    "\n",
    "```python\n",
    "df.to_csv(\"medallero_2024_limpio.csv\", index=False)\n",
    "```\n",
    "\n",
    "> Tip: si en alg√∫n momento `pd.read_html` falla por cambios de la p√°gina, puedes usar `match=...` o `attrs={\"class\":\"wikitable\"}` como en los ejemplos, o inspeccionar `tablas[i].columns` para localizar la correcta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/2024_Summer_Olympics_medal_table\"\n",
    "\n",
    "# Simula un navegador real\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "\n",
    "# Descarga el HTML de la p√°gina\n",
    "response = requests.get(url, headers=headers)\n",
    "response.raise_for_status()  # lanza error si algo falla\n",
    "\n",
    "# Lee todas las tablas del HTML descargado\n",
    "list_of_df = pd.read_html(response.text, header=0)\n",
    "\n",
    "print(len(list_of_df))\n",
    "\n",
    "# Opcional: ver la primera tabla\n",
    "list_of_df[0].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Echa un vistazo a cu√°ntas hay y a sus cabeceras\n",
    "for t in list_of_df:\n",
    "    print(t.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(list_of_df):\n",
    "    print(i, list(t.columns)[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df15=list_of_df[3]\n",
    "df15.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"background-color:green;\">\n",
    "\n",
    "## *Ejercicio 5*\n",
    "4.1. Consulta la tabla de medallas de Paris y recupera los primeros 15 pa√≠ses con m√°s medallas conseguidas. ¬øC√≥mo har√≠as para obtener los 15 √∫ltimos?\n",
    "4.2. Consulta otra p√°gina de tu inter√©s e intenta recuperar los datos almacenados en diferentes tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lectura de datos desde un fichero JSON\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "`read_json` es una funci√≥n de **pandas** que convierte directamente un archivo o cadena en formato **JSON** a un **DataFrame**, de forma muy similar a `read_csv` pero para JSON.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"archivo.json\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üìå Par√°metros principales\n",
    "\n",
    "### 1. **path\\_or\\_buf**\n",
    "\n",
    "* Puede ser:\n",
    "\n",
    "  * Una **ruta a fichero**: `\"datos.json\"`\n",
    "  * Una **URL**: `\"https://.../data.json\"`\n",
    "  * Una **cadena en memoria** con JSON.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **orient**\n",
    "\n",
    "Indica c√≥mo est√° estructurado el JSON y c√≥mo interpretarlo:\n",
    "\n",
    "* `\"records\"` ‚Üí Lista de diccionarios (lo m√°s com√∫n).\n",
    "\n",
    "  ```json\n",
    "  [{\"id\":1,\"edad\":34},{\"id\":2,\"edad\":28}]\n",
    "  ```\n",
    "\n",
    "  ‚Üí DataFrame con columnas `id`, `edad`.\n",
    "\n",
    "* `\"index\"` ‚Üí Diccionario con √≠ndices.\n",
    "\n",
    "  ```json\n",
    "  {\"1\":{\"edad\":34}, \"2\":{\"edad\":28}}\n",
    "  ```\n",
    "\n",
    "  ‚Üí Filas indexadas por 1 y 2.\n",
    "\n",
    "* `\"columns\"` ‚Üí Diccionario por columnas.\n",
    "\n",
    "  ```json\n",
    "  {\"edad\": {\"1\":34,\"2\":28}}\n",
    "  ```\n",
    "\n",
    "  ‚Üí Columna `edad` con √≠ndice 1 y 2.\n",
    "\n",
    "* `\"split\"` ‚Üí Diccionario con keys `\"index\"`, `\"columns\"`, `\"data\"`.\n",
    "  ‚Üí √ötil para exportar/importar entre pandas y JSON.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "```python\n",
    "df = pd.read_json(\"datos.json\", orient=\"records\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **lines**\n",
    "\n",
    "Cuando cada fila del archivo JSON est√° en **una l√≠nea separada** (formato *JSONL*).\n",
    "\n",
    "Archivo `datos.jsonl`:\n",
    "\n",
    "```\n",
    "{\"id\":1,\"edad\":34}\n",
    "{\"id\":2,\"edad\":28}\n",
    "```\n",
    "\n",
    "Lectura:\n",
    "\n",
    "```python\n",
    "df = pd.read_json(\"datos.jsonl\", lines=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **chunksize**\n",
    "\n",
    "Permite leer el JSON por partes (muy √∫til en datasets grandes).\n",
    "\n",
    "```python\n",
    "for chunk in pd.read_json(\"grande.jsonl\", lines=True, chunksize=100000):\n",
    "    print(chunk.shape)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **dtype / convert\\_dates**\n",
    "\n",
    "* `dtype` ‚Üí forzar tipos de columnas.\n",
    "* `convert_dates` ‚Üí intentar convertir a `datetime`.\n",
    "\n",
    "```python\n",
    "df = pd.read_json(\"datos.json\", convert_dates=True)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ruta_json =  \"data/movies.json\"\n",
    "\n",
    "df16 = pd.read_json(ruta_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df16.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df16[df16['title']==\"American Beauty\"]['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_of_americanbeauty=df16[(df16['title']==\"American Beauty\") & (df16['year']==1999)]['actors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(list(cast_of_americanbeauty))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"background-color:green;\">\n",
    "\n",
    "## *Ejercicio 6*\n",
    "Consulta el repositorio de [Public web APIs de Todd Motto](https://github.com/public-apis/public-apis) y desc√°rgate un JSON de tu inter√©s. Consulta a ver qu√© datos puedes recuperar.\n",
    "\n",
    "Si quieres, tambi√©n puedes hacer una petici√≥n HTTP request para recuperar el fichero online con la siguiente notaci√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get('https://anapioficeandfire.com/api/books')\n",
    "response_data = response.json()\n",
    "response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df17 = pd.DataFrame.from_dict(response_data)\n",
    "df17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# üìå 7. Lectura de datos desde una base de datos\n",
    "\n",
    "### 1) Conexi√≥n a bases de datos SQL con `pandas.read_sql`\n",
    "\n",
    "`pandas` permite ejecutar consultas SQL directamente y obtener los resultados como un `DataFrame`.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import sqlite3  # Ejemplo con SQLite, pero puede ser cualquier motor SQL\n",
    "\n",
    "# 1. Crear conexi√≥n\n",
    "conexion = sqlite3.connect(\"mi_base.db\")\n",
    "\n",
    "# 2. Leer tabla completa\n",
    "df = pd.read_sql(\"SELECT * FROM clientes\", conexion)\n",
    "\n",
    "# 3. O ejecutar una consulta\n",
    "df_filtrado = pd.read_sql(\"SELECT nombre, edad FROM clientes WHERE edad > 30\", conexion)\n",
    "\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Usando **SQLAlchemy** (m√°s flexible)\n",
    "\n",
    "SQLAlchemy permite conectar a distintos motores: SQLite, MySQL, PostgreSQL, SQL Server, Oracle‚Ä¶\n",
    "\n",
    "```python\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Ejemplo con PostgreSQL\n",
    "engine = create_engine(\"postgresql+psycopg2://usuario:password@localhost:5432/mibase\")\n",
    "\n",
    "# Leer una tabla\n",
    "df = pd.read_sql(\"SELECT * FROM ventas\", engine)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3) Conexi√≥n a MySQL o MariaDB\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\"mysql+pymysql://usuario:password@localhost:3306/mibase\")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM productos\", engine)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4) Conexi√≥n a bases de datos grandes\n",
    "\n",
    "Cuando la tabla es enorme:\n",
    "\n",
    "* Usa consultas SQL con `WHERE` o `LIMIT`.\n",
    "* Procesa en **chunks**:\n",
    "\n",
    "```python\n",
    "query = \"SELECT * FROM logs\"\n",
    "for chunk in pd.read_sql(query, engine, chunksize=50000):\n",
    "    print(chunk.shape)\n",
    "    # procesar por partes\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5) Bases NoSQL\n",
    "\n",
    "* **MongoDB** ‚Üí se conecta con `pymongo` y luego se transforma en DataFrame:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"mi_base\"]\n",
    "coleccion = db[\"usuarios\"]\n",
    "\n",
    "datos = list(coleccion.find({}, {\"_id\":0}))  # quitar el campo _id\n",
    "df = pd.DataFrame(datos)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    " **Resumen**\n",
    "\n",
    "* `pd.read_sql(query, conexion)` ‚Üí para SQL cl√°sico.\n",
    "* `SQLAlchemy` ‚Üí conexi√≥n universal a muchos motores.\n",
    "* `chunksize` ‚Üí datasets grandes.\n",
    "* Para NoSQL (ej. MongoDB), primero obt√©n un `dict/list` y luego convi√©rtelo con `pd.DataFrame`.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# cargamos la base de datos de https://www.kaggle.com/datasets/shahjhanalam/movie-data-analytics-dataset?resource=download\n",
    "ruta_sqlite = os.getcwd() + \"/data/movie.sqlite\"\n",
    "\n",
    "# 1. Crear conexi√≥n a un archivo SQLite (si no existe se crea)\n",
    "conexion = sqlite3.connect(ruta_sqlite)\n",
    "\n",
    "# 2. Consultar tablas disponibles\n",
    "tablas = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table';\", conexion)\n",
    "print(tablas)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Leer una tabla\n",
    "df_imdb = pd.read_sql(\"SELECT * FROM IMDB\", conexion)\n",
    "print(df_imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:green;\">\n",
    "\n",
    "## *Ejercicio 6*\n",
    "\n",
    "6.1. Consulta la base de datos y muestras las pel√≠culas m√°s taquilleras en el mundo\n",
    "\n",
    "6.2. Consulta cual es el g√©nero m√°s popular de la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 8. ¬øY d√≥nde encontrar datasets interesantes para an√°lisis de datos?\n",
    "\n",
    "---\n",
    "\n",
    "##  Repositorios generales\n",
    "\n",
    "* **[Kaggle](https://www.kaggle.com/datasets)** ‚Üí Miles de datasets en m√∫ltiples formatos (CSV, JSON, SQLite, im√°genes‚Ä¶).\n",
    "* **[Google Dataset Search](https://datasetsearch.research.google.com/)** ‚Üí Buscador universal de datasets p√∫blicos.\n",
    "* **[Data.world](https://data.world/)** ‚Üí Comunidad colaborativa de datos.\n",
    "\n",
    "---\n",
    "\n",
    "##  Repositorios acad√©micos y de ML\n",
    "\n",
    "* **[UCI ML Repository](https://archive.ics.uci.edu/ml/index.php)** ‚Üí Uno de los m√°s cl√°sicos en machine learning (Iris, Wine, Adult, etc.).\n",
    "* **[Papers with Code ‚Äì Datasets](https://paperswithcode.com/datasets)** ‚Üí Datasets asociados a papers de investigaci√≥n.\n",
    "* **[OpenML](https://www.openml.org/)** ‚Üí Colecci√≥n de datasets preparados para ML, listos para descargar y usar.\n",
    "\n",
    "---\n",
    "\n",
    "##  Recopilaciones\n",
    "\n",
    "* **[GitHub Awesome Datasets](https://github.com/awesomedata/awesome-public-datasets)** ‚Üí Gran lista categorizada por dominios (clima, biolog√≠a, econom√≠a, im√°genes, NLP, etc.).\n",
    "* **[FiveThirtyEight Datasets](https://data.fivethirtyeight.com/)** ‚Üí Datos de art√≠culos period√≠sticos (pol√≠tica, deportes, sociedad).\n",
    "\n",
    "---\n",
    "\n",
    "##  Datos abiertos de instituciones\n",
    "\n",
    "* **[data.gov](https://www.data.gov/)** ‚Üí Portal de datos abiertos de EE. UU.\n",
    "* **[datos.gob.es](https://datos.gob.es/)** ‚Üí Portal oficial de Espa√±a.\n",
    "* **[World Bank Data](https://data.worldbank.org/)** ‚Üí Indicadores econ√≥micos y sociales globales.\n",
    "* **[Eurostat](https://ec.europa.eu/eurostat/)** ‚Üí Estad√≠sticas europeas.\n",
    "* **[UN Data](https://data.un.org/)** ‚Üí Datos de Naciones Unidas.\n",
    "\n",
    "---\n",
    "\n",
    "##  Datasets tem√°ticos y divertidos\n",
    "\n",
    "* **[IMDb Datasets](https://www.imdb.com/interfaces/)** ‚Üí Informaci√≥n sobre cine y series.\n",
    "* **[Spotify Dataset en Kaggle](https://www.kaggle.com/datasets)** ‚Üí M√∫sica y playlists.\n",
    "* **[Sports Reference](https://www.sports-reference.com/)** ‚Üí Estad√≠sticas de deportes (NBA, NFL, MLB, etc.).\n",
    "* **[Pok√©mon Dataset (Kaggle)](https://www.kaggle.com/abcsds/pokemon)** ‚Üí Informaci√≥n de Pok√©mon üòÑ.\n",
    "* **[Video Game Sales (Kaggle)](https://www.kaggle.com/gregorut/videogamesales)** ‚Üí Ventas globales de videojuegos.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[Kaggle](https://www.kaggle.com/datasets)\n",
    "[Github AwesomeDatasets](https://github.com/awesomedata/awesome-public-datasets)\n",
    "[Papers with code](https://paperswithcode.com/datasets)\n",
    "[UCI (University of California Irvine) ML repository](https://archive.ics.uci.edu/ml/index.php)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Webgraf√≠a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "##  Documentaci√≥n oficial\n",
    "\n",
    "* **[Pandas ‚Äì IO Tools (Input/Output)](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html)**\n",
    "  Explicaci√≥n completa de todas las funciones de entrada/salida: `read_csv`, `read_json`, `read_sql`, `read_excel`, etc.\n",
    "* **[Pandas API Reference ‚Äì Input/Output](https://pandas.pydata.org/pandas-docs/stable/reference/io.html)**\n",
    "  Lista detallada de par√°metros y ejemplos de uso.\n",
    "\n",
    "---\n",
    "\n",
    "## Cursos y tutoriales\n",
    "\n",
    "* **[Kaggle Learn: Pandas](https://www.kaggle.com/learn/pandas)**\n",
    "  Curso gratuito y pr√°ctico, con notebooks interactivos.\n",
    "* **[W3Schools ‚Äì Pandas Read Files](https://www.w3schools.com/python/pandas/pandas_ref_io.asp)**\n",
    "  Ejemplos r√°pidos de importaci√≥n desde CSV, JSON, Excel, SQL.\n",
    "* **[GeeksforGeeks ‚Äì Reading data in Pandas](https://www.geeksforgeeks.org/python-pandas-dataframe/)**\n",
    "  Tutorial con m√∫ltiples ejemplos de carga de datos.\n",
    "* **[Real Python ‚Äì Pandas Tutorials](https://realpython.com/search?q=pandas)**\n",
    "  Art√≠culos pr√°cticos sobre carga y an√°lisis de datos.\n",
    "\n",
    "---\n",
    "##  Repositorios y notebooks de pr√°ctica\n",
    "\n",
    "* **[Awesome Public Datasets (GitHub)](https://github.com/awesomedata/awesome-public-datasets)**\n",
    "  Gran recopilaci√≥n de datasets para practicar importaci√≥n en distintos formatos.\n",
    "* **[UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)**\n",
    "  Fuente cl√°sica de datasets para practicar con `read_csv` y `read_table`.\n",
    "* **[Papers with Code ‚Äì Datasets](https://paperswithcode.com/datasets)**\n",
    "  Colecci√≥n de datasets modernos asociados a papers de ML.\n",
    "* **[Kaggle Datasets](https://www.kaggle.com/datasets)**\n",
    "  Miles de datasets listos para probar en notebooks con pandas.\n",
    "\n",
    "---\n",
    "\n",
    "##  Videos y MOOCs\n",
    "\n",
    "* **[freeCodeCamp ‚Äì Pandas Tutorial (YouTube)](https://www.youtube.com/watch?v=vmEHCJofslg)**\n",
    "  Curso completo de 4 horas con ejemplos de carga de datos.\n",
    "* **[Coursera ‚Äì Data Analysis with Python](https://www.coursera.org/learn/data-analysis-with-python)**\n",
    "  Curso oficial de IBM, con m√≥dulos sobre importaci√≥n de datos.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Licencia\n",
    "\n",
    "[![Licencia: CC BY-SA 4.0](https://licensebuttons.net/l/by-sa/4.0/88x31.png)](https://creativecommons.org/licenses/by-sa/4.0/)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
